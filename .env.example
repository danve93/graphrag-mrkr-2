# Environment Configuration Template
# Copy this file to .env and update with your actual values

# LLM / Ollama Configuration
# Set provider to 'ollama' to use a local Ollama server for embeddings and LLM
# Supported values: 'openai' (default) or 'ollama'
LLM_PROVIDER=openai

# OpenAI API Configuration
OPENAI_API_KEY=your_openai_api_key_here
OPENAI_BASE_URL=https://api.openai.com/v1/
OPENAI_MODEL=gpt-4o-mini
OPENAI_PROXY=

# Gemini Configuration
# Get your API key from Google AI Studio
# GEMINI_API_KEY=your_key_here
# GEMINI_MODEL=gemini-3.0-flash
# GEMINI_EMBEDDING_MODEL=models/text-embedding-004

# Neo4j Configuration
# Use bolt://neo4j:7687 for Docker Compose, bolt://localhost:7687 for local Python scripts
NEO4J_URI=bolt://localhost:7687
NEO4J_USERNAME=neo4j
NEO4J_PASSWORD=your_neo4j_password_here
NEO4J_AUTH=NEO4J_USERNAME/NEO4J_PASSWORD
# Optional Docker Compose credential shortcut: set NEO4J_AUTH to '<user>/<password>'
# If set, docker-compose will use NEO4J_AUTH (e.g. NEO4J_AUTH=neo4j/test). When running
# the backend/service locally outside Compose, keep NEO4J_USERNAME/NEO4J_PASSWORD set for the
# application to read.
# Example for local demo (DO NOT USE in CI or public repos):
# NEO4J_AUTH=neo4j/test  # short/demo password; prefer generating a secure password or set NEO4J_USERNAME/NEO4J_PASSWORD

# If using Ollama, set the base URL (e.g. http://host.docker.internal:11434 or http://ollama:11434)
#OLLAMA_BASE_URL=http://localhost:11434
#OLLAMA_MODEL=llama3.2
#OLLAMA_EMBEDDING_MODEL=nomic-embed-text
#OLLAMA_API_KEY=   # Not required for local Ollama server

# Embedding Configuration
EMBEDDING_MODEL=text-embedding-ada-002
EMBEDDING_CONCURRENCY=3

# Document Processing Configuration (Optimized for Company Docs)
CHUNK_SIZE=1200
CHUNK_OVERLAP=150
CHUNK_TARGET_TOKENS=800
CHUNK_MIN_TOKENS=180
CHUNK_MAX_TOKENS=1000
CHUNK_OVERLAP_TOKENS=100
CHUNK_TOKENIZER=cl100k_base
CHUNK_INCLUDE_HEADING_PATH=true
CHUNKER_STRATEGY_PDF=docling_hybrid
CHUNKER_STRATEGY_HTML=html_heading

# Application Configuration
LOG_LEVEL=INFO
MAX_UPLOAD_SIZE=104857600

# Feature Flags (Optimized for Quality)
ENABLE_ENTITY_EXTRACTION=true
ENABLE_QUALITY_SCORING=true
ENABLE_DELETE_OPERATIONS=true
FLASHRANK_ENABLED=true
FLASHRANK_MODEL_NAME=ms-marco-TinyBERT-L-2-v2

# Microsoft GraphRAG Phases (Enabled for Quality)
ENABLE_GLEANING=true
MAX_GLEANINGS=1
ENABLE_PHASE2_NETWORKX=true
ENABLE_DESCRIPTION_SUMMARIZATION=true

# Document Conversion (auto|native|marker|docling)
DOCUMENT_CONVERSION_PROVIDER=auto

# Marker PDF Conversion (Highest Accuracy)
USE_MARKER_FOR_PDF=true
MARKER_USE_LLM=true
MARKER_FORCE_OCR=true
MARKER_OUTPUT_FORMAT=markdown
MARKER_LLM_MODEL=gpt-4o-mini
# SECURITY: API keys must ONLY be set here in .env, never in JSON config or code
# Optional: Separate API key for Marker (defaults to OPENAI_API_KEY if not set)
# MARKER_LLM_API_KEY=your_openai_api_key_here
# Optional: Set MARKER_LLM_SERVICE to use different LLM service class
# MARKER_LLM_SERVICE=marker.services.openai.OpenAIService

# Caching Configuration
ENTITY_LABEL_CACHE_SIZE=5000
ENTITY_LABEL_CACHE_TTL=300
EMBEDDING_CACHE_SIZE=10000
RETRIEVAL_CACHE_SIZE=1000
RETRIEVAL_CACHE_TTL=60
RESPONSE_CACHE_SIZE=2000
RESPONSE_CACHE_TTL=300
NEO4J_MAX_CONNECTION_POOL_SIZE=50
ENABLE_CACHING=true
# New CacheService Configuration
CACHE_TYPE=disk
CACHE_DIR=data/cache
EMBEDDING_CACHE_TTL=604800  # 7 days
RESPONSE_CACHE_TTL=7200     # 2 hours

# Additional Marker options (advanced)
MARKER_PAGINATE_OUTPUT=true
MARKER_STRIP_EXISTING_OCR=false
MARKER_PDFTEXT_WORKERS=4

# Redis Configuration (optional, for background job processing)
# REDIS_URL=redis://localhost:6379/0

# ========================================
# Retrieval & Fusion Configuration
# ========================================

# Reciprocal Rank Fusion (RRF)
# Combines ranked lists from vector, entity, keyword, and path-based retrieval
# Recommended: Enable for comparative/analytical queries
ENABLE_RRF=false
RRF_K=60

# Chunk Full-Text Search (BM25)
# Enables keyword search for exact-term matching (IDs, versions, legal clauses)
# Recommended: Always enabled for production
ENABLE_CHUNK_FULLTEXT=true
KEYWORD_SEARCH_WEIGHT=0.3

# Query Expansion
# Automatically expands queries with synonyms when results < threshold
# Trade-off: Improved recall but adds LLM latency (~200-500ms)
ENABLE_QUERY_EXPANSION=false
QUERY_EXPANSION_THRESHOLD=3

# Hybrid Retrieval Weights
# Balance between chunk-based (semantic) and entity-based (graph) retrieval
# Adjusted automatically by query routing based on query type
HYBRID_CHUNK_WEIGHT=0.6
HYBRID_ENTITY_WEIGHT=0.4

# ========================================
# Reranking Configuration
# ========================================

# FlashRank Post-Retrieval Reranking
# Uses cross-encoder to refine candidate ordering
# Models: ms-marco-TinyBERT-L-2-v2 (fast), ms-marco-MiniLM-L-6-v2 (balanced)
FLASHRANK_ENABLED=true
FLASHRANK_MODEL_NAME=ms-marco-TinyBERT-L-2-v2
FLASHRANK_MAX_CANDIDATES=100
FLASHRANK_BLEND_WEIGHT=0.0
FLASHRANK_BATCH_SIZE=32
FLASHRANK_PREWARM_IN_PROCESS=true

# ========================================
# Graph & Clustering Configuration
# ========================================

# Graph Expansion
# Follow entity relationships and chunk similarities during retrieval
ENABLE_GRAPH_EXPANSION=true
MAX_EXPANDED_CHUNKS=500
MAX_EXPANSION_DEPTH=2
EXPANSION_SIMILARITY_THRESHOLD=0.1

# Leiden Community Detection
# Groups related entities into semantic clusters for visualization
ENABLE_CLUSTERING=true
ENABLE_GRAPH_CLUSTERING=true
CLUSTERING_RESOLUTION=1.0
CLUSTERING_MIN_EDGE_WEIGHT=0.0

# ========================================
# Entity Extraction & Processing
# ========================================

# Entity Extraction Master Toggle
ENABLE_ENTITY_EXTRACTION=true

# Gleaning (Multi-Pass Extraction)
# Additional passes to capture missed entities (Microsoft GraphRAG approach)
# MAX_GLEANINGS=1 means 2 total passes (initial + 1 gleaning)
ENABLE_GLEANING=true
MAX_GLEANINGS=1

# NetworkX Intermediate Layer (Phase 2)
# Deduplicates entities before Neo4j persistence (22% reduction)
ENABLE_PHASE2_NETWORKX=true
NEO4J_UNWIND_BATCH_SIZE=500
MAX_NODES_PER_DOC=2000
MAX_EDGES_PER_DOC=5000

# Description Summarization (Phase 4)
# LLM-based summarization for verbose entity descriptions (50-70% reduction)
ENABLE_DESCRIPTION_SUMMARIZATION=true
SUMMARIZATION_MIN_MENTIONS=3
SUMMARIZATION_MIN_LENGTH=200
SUMMARIZATION_BATCH_SIZE=5
SUMMARIZATION_CACHE_ENABLED=true

# Tuple Format Configuration (Phase 3)
ENTITY_EXTRACTION_FORMAT=tuple_v1
TUPLE_FORMAT_VALIDATION=true
TUPLE_MAX_DESCRIPTION_LENGTH=500

# ========================================
# Document Processing Configuration
# ========================================

# Quality Scoring & Filtering
ENABLE_QUALITY_SCORING=true
ENABLE_QUALITY_FILTERING=true

# Document Summaries (Precomputed)
ENABLE_DOCUMENT_SUMMARIES=true
DOCUMENT_SUMMARY_TTL=300

# ========================================
# Advanced Features
# ========================================

# LLM Streaming (Provider-Level)
# Experimental: Stream tokens directly from LLM provider
ENABLE_LLM_STREAMING=false

# Delete Operations
ENABLE_DELETE_OPERATIONS=true

# ========================================
# TruLens Continuous Monitoring (Optional)
# ========================================

# Enable TruLens continuous monitoring for RAG pipeline quality tracking
# Set to "1" to enable, "0" or unset to disable
# Requires: uv pip install -r evals/trulens/requirements-trulens.txt
ENABLE_TRULENS_MONITORING=0

# TruLens PostgreSQL Database Configuration
# Used for storing monitoring data, feedback evaluations, and metrics
# Start PostgreSQL: docker compose --profile trulens up -d
TRULENS_DB_HOST=localhost
TRULENS_DB_PORT=5433
TRULENS_DB_NAME=trulens
TRULENS_DB_USER=postgres
TRULENS_DB_PASSWORD=trulens_password

# For Docker Compose internal networking, use:
# TRULENS_DB_HOST=postgres-trulens
# POSTGRES_HOST=postgres-trulens

# TruLens Configuration
# Sampling rate: 1.0 = 100% of requests, 0.1 = 10% (reduces LLM API costs)
TRULENS_SAMPLING_RATE=1.0

# Dashboard port (TruLens Streamlit dashboard)
TRULENS_DASHBOARD_PORT=8501

# Prometheus metrics endpoint (exposed via FastAPI)
# Access at: http://localhost:8000/api/trulens/metrics
TRULENS_METRICS_ENABLED=true
