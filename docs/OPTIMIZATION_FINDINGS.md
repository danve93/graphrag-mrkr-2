# Optimization Findings — Streaming, Caching, and Retrieval

Purpose: record factual findings from a codebase scan related to streaming, caching, retrieval, reranking, and metadata filtering. This document lists the goals (as described in the optimization ideas) and the exact, evidence-backed current state in the repository. It does not assume behavior beyond what is found in code or documentation files.

---

**How to read this doc**
- "Goal" statements come from the optimization ideas. They are presented as goals only.
- "Current implementation (facts)" lists exact code locations and observed behavior found in the repository.
- "Gap / Status" is a factual statement whether the goal is implemented, partially implemented, or not implemented, based on code inspection.

---

## 1) Stream Output Tokens

Goal
- Stream tokens as they’re generated from the LLM so the client receives tokens as soon as the model produces them (reducing Time To First Token).

Current implementation (facts)
- The backend exposes SSE endpoints that stream events to clients: see `api/routers/chat.py`.
  - `stream_response_generator(...)` yields SSE events of types `stage`, `token`, `sources`, `quality_score`, `follow_ups`, `metadata`, and `done`.
  - `POST /api/chat/stream` and `POST /api/chat/query` return `StreamingResponse` when `stream=True` (file: `api/routers/chat.py`).
- LLM calls in `core/llm.py` are non-streaming:
  - OpenAI usage calls `openai.chat.completions.create(...)` without a streaming parameter in `_generate_openai_response` / `_generate_openai_response_with_history`.
  - Ollama calls use `requests.post(..., json={... , "stream": False})` in `_generate_ollama_response` and `_generate_ollama_response_with_history` (file: `core/llm.py`).
- The SSE token events currently come from pre-generated text: `api/routers/chat.py` obtains `result.get("response", "")` and streams it word-by-word; the generator builds tokens from that full `response` string rather than forwarding model-produced tokens in real time.

Gap / Status
- Partial: SSE streaming to clients is implemented and functional (`api/routers/chat.py`), but token streaming currently only starts after the model response has been fully generated by the LLM (no direct LLM streaming integration present in `core/llm.py`).

Files referenced
- `api/routers/chat.py` (stream generator and endpoints)
- `core/llm.py` (LLM methods; no streaming API usage)

---

## 2) Add Semantic Caching (response-level caching)

Goal
- Cache LLM responses (or full pipeline responses) for semantically similar queries to reduce response time for repeated patterns (e.g., FAQs).

Current implementation (facts)
- There are multiple caches implemented as singletons in `core/singletons.py`:
  - `get_embedding_cache()` returns an `LRUCache` keyed by `hash_text(text, model)` (no TTL).
  - `get_entity_label_cache()` returns a `TTLCache` (entity labels, TTL configured).
  - `get_retrieval_cache()` returns a `TTLCache` used for retrieval results (TTL configured, short TTL).
- `rag/retriever.py::DocumentRetriever.hybrid_retrieval()` generates a cache key using `hash_retrieval_params(...)` and stores retrieval results in the retrieval cache (file: `rag/retriever.py` and `core/singletons.py`).
- `core/cache_metrics.py` collects hit/miss counters for entity label, embedding, and retrieval caches.
- I found no code that caches the full RAG/LLM response (a combined `response` + `sources` + `metadata`) keyed by a semantic hash of the query and retrieval context.

Gap / Status
- Partial: embedding-level and retrieval-level caching exist and are used, but there is no response-level (semantic) cache implemented in the codebase.

Files referenced
- `core/singletons.py` (embedding, entity label, retrieval caches)
- `rag/retriever.py` (retrieval cache usage)
- `core/cache_metrics.py` (metrics)

---

## 3) Prompt Caching / KV cache optimization

Goal
- Place static prompt content (system instructions/templates) first and dynamic user content last to exploit prompt-level caches (provider-specific) and avoid re-sending large static text repeatedly.

Current implementation (facts)
- `core/llm.py` constructs a `system_message` string and a `prompt` that includes context and question. The full prompt is assembled and passed to `openai.chat.completions.create(...)` or to Ollama's generate endpoint in one call.
- There is no code implementing a specialized prompt-caching layer or provider-specific prompt cache keys. No `prompt_cache` or similar singleton was found.

Gap / Status
- Not implemented: prompt-level caching or use of a prompt-cache key mechanism is not present. The system message and prompt are constructed per request and sent in standard calls (file: `core/llm.py`).

Files referenced
- `core/llm.py` (prompt & system message construction)

---

## 4) Hybrid Search is Mandatory: BM25 (keyword) + Vector Search

Goal
- Combine lexical/keyword search (BM25-like) with vector semantics for better precision on exact matches and concepts. Implement both chunk-level keyword search and vector search, and combine results.

Current implementation (facts)
- Vector search for chunks is implemented in `core/graph_db.py::vector_similarity_search(query_embedding, top_k)`. This uses GDS cosine similarity over `c.embedding` in Neo4j.
- Entity-level full-text search is implemented: `core/graph_db.py::entity_similarity_search(query_text, top_k)` creates/uses a Neo4j full-text index for `Entity` nodes and calls `CALL db.index.fulltext.queryNodes('entity_text', $query_text)` to rank entities by keyword similarity.
- There is no function exposed that performs full-text/BM25 search over `Chunk` content (no `chunk_keyword_search` found). Chunk retrieval is vector-only (for chunk content) and entity fulltext covers only `Entity` nodes.

Gap / Status
- Partial: vector chunk search is implemented; entity-level full-text is implemented. Chunk-level lexical search (BM25/full-text on chunk content) is not present in the inspected code.

Files referenced
- `core/graph_db.py::vector_similarity_search`
- `core/graph_db.py::entity_similarity_search`
- `rag/retriever.py` (uses chunk retrieval & entity searches)

---

## 5) Reciprocal Rank Fusion (RRF)

Goal
- Combine ranking lists (keyword and vector and other sources) using Reciprocal Rank Fusion (RRF) rather than naive score averaging to produce a robust fused ranking.

Current implementation (facts)
- `rag/retriever.py::hybrid_retrieval()` merges chunk-based, entity-based, and path-based candidates. The merging logic computes weighted hybrid scores based on configured `chunk_fraction`, `entity_fraction`, and `path_fraction`. The code applies weight-based blending and boosting for duplicates.
- I did not find a dedicated RRF implementation (no function named `rrf` or `reciprocal_rank_fusion` and no RRF formula applied in the code).

Gap / Status
- Not implemented: explicit RRF is not present; the code uses a weighted score merge approach instead (file: `rag/retriever.py`).

Files referenced
- `rag/retriever.py` (hybrid merging & weighting)

---

## 6) Re-Ranking (retrieve many, then cross-encoder rerank)

Goal
- Retrieve a larger candidate set (e.g., 50) and apply a cross-encoder reranker (e.g., BGE-Reranker) to produce more accurate ordering, then pass top results to generation.

Current implementation (facts)
- There is an optional FlashRank reranker wrapper in `rag/rerankers/flashrank_reranker.py`:
  - `rerank_with_flashrank(query, candidates, max_candidates)` accepts candidates and reorders them using FlashRank if `settings.flashrank_enabled` is true.
  - `prewarm_ranker()` and a prewarm worker script `scripts/flashrank_prewarm_worker.py` exist.
- `rag/retriever.py::hybrid_retrieval()` calls `rerank_with_flashrank` when `settings.flashrank_enabled` is true and blends scores according to `flashrank_blend_weight`.
- The pipeline supports a reranker but I did not find a separate cross-encoder/BGE-specific reranker module other than the FlashRank wrapper; FlashRank may be the project’s chosen reranker (it is optional and lazily initialized in `rag/rerankers/flashrank_reranker.py`).

Gap / Status
- Partially implemented: an optional FlashRank reranker exists and is invoked by the retriever. There is no explicit separate cross-encoder (BGE) reranker module in the codebase beyond the FlashRank wrapper.

Files referenced
- `rag/rerankers/flashrank_reranker.py` (reranker wrapper)
- `scripts/flashrank_prewarm_worker.py` (prewarm worker)
- `rag/retriever.py` (invokes reranker)

---

## 7) Metadata Filtering (pre-filter by year/category/source)

Goal
- Allow narrowing the search space by metadata before vector search runs (pre-filter documents by attributes like year, category, source) to improve precision and reduce search cost.

Current implementation (facts)
- API request model `ChatRequest` includes `context_documents` and `context_hashtags` fields (`api/models.py`). The chat router passes `context_documents` into the RAG pipeline (`api/routers/chat.py`).
- The retriever accepts `allowed_document_ids` / `context_documents` and applies document-level restrictions via `_filter_chunks_by_documents(...)` in `rag/retriever.py`.
- `core/graph_db.py` stores document properties (hashtags, summaries) via `update_document_summary()` and `update_document_hashtags()` and provides `get_documents_with_summaries()` and `get_all_hashtags()`.
- I did not find a generic helper that converts arbitrary metadata filters (e.g., `year >= 2020 AND category = 'legal'`) into `allowed_document_ids` automatically; the existing code supports explicit lists of document IDs and hashtags passed from the client.

Gap / Status
- Partial: document-level restriction support exists (explicit `context_documents` lists and hashtag fields). Automatic metadata-to-document-ID pre-filtering utilities are not present in the inspected code.

Files referenced
- `api/models.py` (request fields)
- `api/routers/chat.py` (passes context_documents)
- `rag/retriever.py` (document restriction and `_filter_chunks_by_documents`)
- `core/graph_db.py` (document metadata methods)

---

## Other relevant factual notes
- The RAG pipeline is implemented as a LangGraph StateGraph in `rag/graph_rag.py`. Pipeline stages `query_analysis`, `retrieval`, `graph_reasoning`, and `generation` are tracked in `state['stages']` and emitted to clients via SSE (`api/routers/chat.py`).
- Embedding generation has retry/backoff decorators and rate-limit pacing (`core/embeddings.py`). Embeddings are cached using the embedding singleton cache (`core/singletons.py`).
- Cache metrics are implemented in `core/cache_metrics.py` and report hits/misses for entity labels, embeddings, and retrieval caches.

Files referenced (general)
- `rag/graph_rag.py` (pipeline)
- `core/embeddings.py` (embedding manager)
- `core/singletons.py` (caches, driver singleton)
- `core/cache_metrics.py` (metrics)

---

## Summary table (goal → current state)
- Stream tokens from LLM: SSE streaming to clients exists, but LLM streaming is not implemented in `core/llm.py` (partial).
- Semantic response caching: embedding/retrieval/entity caches exist; no response-level semantic cache found (not implemented).
- Prompt caching / KV prompt optimization: not implemented; prompts are built per-request in `core/llm.py` (not implemented).
- Hybrid search (BM25 + vector): vector chunk search and entity full-text exist; chunk-level lexical/BM25 over `Chunk` content is not present (partial).
- Reciprocal Rank Fusion: not present; system uses weight-based blending in `rag/retriever.py` (not implemented).
- Re-ranking with cross-encoder: FlashRank wrapper exists and is used optionally; explicit cross-encoder reranker module (e.g., BGE-reranker) not found (partial).
- Metadata pre-filter: explicit `context_documents`/`context_hashtags` are supported; generic metadata-to-document-id pre-filter helper not found (partial).

---

File created
- This file: `docs/OPTIMIZATION_FINDINGS.md` (this path)

---

If you want, I can now (pick one):
- Add a response-level cache implementation (low-risk change). OR
- Implement true LLM streaming integration in `core/llm.py` and wire it into the SSE generator (larger change). OR
- Add chunk-level full-text search and a Reciprocal Rank Fusion merge in the retriever (medium change).

I did not make any changes to code behavior in this run; the document above is a factual transcription of what I found in the repository files referenced.
